import logging
import unicodedata
from scipy import sparse

import numpy
import regex
from sklearn.utils.murmurhash import murmurhash3_32

logger = logging.getLogger(__name__)

# ------------------------------------------------------------------------------
# 数据清洗相关
# ------------------------------------------------------------------------------
from tqa.retriever.tokens import Tokens

STOPWORDS_EN = {
    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your',
    'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she',
    'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',
    'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that',
    'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',
    'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of',
    'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through',
    'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',
    'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then',
    'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',
    'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor',
    'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can',
    'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've',
    'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven',
    'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren',
    'won', 'wouldn', "'ll", "'re", "'ve", "n't", "'s", "'d", "'m", "''", "``"
}

STOPWORDS_ZH = {
    '格外', '起头', '大略', '如果', '豁然', '更', '不满', 'Ⅲ', '日复一日', '顷刻之间', '这', '/', '(', '简言之', '亲口', '这么', '届时', '啊哈', '但是', '趁', '别', '故', '则', '均', '朝', '基本上', '及其', '自己', '较比', '每当', '那里', '从未', '除开', '略加', '平素', '遵照', '差一点', '彼此', '宁', '即使', '累年', '权时', '从今以后', '快', '哪个', '迄', '然', '±', '除外', '不必', '三番两次', '按期', '光', '所以', '（', '%', '连同', '［］', '绝不', '几度', '岂', '乘隙', '立地', '哪边', '诚然', '吓', '例如', '同时', '向着', '没', '偶而', '啦', '顿时', '～', '好在', '累次', '话说', '绝', '谁', '▲', '的话', '必将', '到底', '临', '某', '不要', '乘', '.', '」', '如', '极度', '其实', '乌乎', '同', '呕', '〕〔', '哎', '依照', '几时', '嗡嗡', '总之', '尔后', '当场', '次第', '后来', '起先', 'γ', '在下', '呃', '若', '之类', '较', '极其', '长线', '之一', '一方面', '屡屡', '这时', '＠', '吗', '因而', '这些', '比如', '于', '！', '两者', '除此', '那么', '不比', '时候', '不免', '偶尔', '难得', '据说', '怎么样', '呜呼', '怪不得', '加之', '沿', '不然的话', '前者', '紧接着', '当真', '大不了', '汝', '啊呀', '甚么', '本身', '亲手', '得', '其它', '迫于', '不止一次', '勃然', '被', '不但', '本', '究竟', '【', '倒不如', '如上所述', '当庭', '无论', '奇', '第', 'φ', '精光', '哟', '宁愿', '换句话说', '并无', '这会儿', '初', '为', '何须', '猛然间', '然则', '奈', '哪儿', '非常', 'μ', '将', '谁知', '决非', '呸', '隔日', '有的', '吧', '这边', '不得已', '...', '各个', '据悉', '九', '当着', '连日来', '还有', '牢牢', '我', '&', '不知不觉', '即', '。', '而已', '毫无保留地', '略微', '饱', '也罢', '＿', '如若', '〉', '大家', '接下来', '很', '哪年', '许多', '彻夜', '难道', '以至', '大张旗鼓', '方', '当儿', '_', '不怎么', '叫', '他们', '老老实实', '刚巧', '并排', '过', '绝非', '据此', '哪怕', '）', '常常', '在', '为了', '怕', '如今', '′', '放量', '四', '莫', '而', '呀', '敞开儿', '进去', '看样子', '进而', '大抵', '另外', '这儿', '矣', '川流不息', '更加', '哈哈', '怎', '以便', '成心', '不仅仅', '纵令', '并', '挨家挨户', '暗地里', '不会', '〔', '以至于', '啪达', '挨着', '从宽', '而又', "'", '及', '也就是说', '这样', '抽冷子', '怎么', '地', '赶快', '请勿', '怪', '也好', '归根结底', '大致', '一则', '不可开交', '得天独厚', '以及', '拦腰', '挨门逐户', '甚至', '管', '不外乎', '顺着', '恰恰', '分期', '千', '立刻', '大大', '可以', '此中', '除了', '二话没说', '单单', '莫若', '不单', '不特', '按', '关于', '≈', '那末', '零', '故此', '于是乎', '若非', '不惟', '6', '起见', '不消', '倘然', '个', '仅', '切勿', '万一', '上去', '边', '>>', '·', '固', '开外', '咱', '乘胜', '岂但', '每每', '？', '如何', '们', '不迭', '与其', '毕竟', '就是说', '与此同时', '动不动', '尽量', '云云', '毋宁', '其次', '＞', '到了儿', '凭', '甚而', '不同', '然而', '∈', '常言说', '全身心', '据实', '虽然', '概', '常', '独自', '亲自', '内', '冲', '从此', '出', '就', '得起', '总的说来', '可是', '倘或', '她们', '屡次', '猛然', '挨个', '日渐', '反倒', '弗', '与否', '由此可见', '各位', '不妨', '背靠背', '以免', '】', '抑或', '一直', '顺', '经', '某个', '几番', '哎呀', '这个', '而外', '至', '焉', '多亏', '那个', '将近', '.一', '趁着', '仍', '多次', '嘻', '出去', '`', '不可抗拒', '尽管', '连连', '趁早', '吱', '来着', '极了', '差不多', '怎样', '活', '纵然', '匆匆', '以致', '顷刻间', '哇', '另一个', '不如', '＇', '每逢', '种', '多多', '切', '开始', '率尔', '毫不', '要不是', '了', '＆', '以', '殆', '从轻', '路经', '你们', '无宁', '为着', '不料', '上', '哉', '几经', '分头', '充分', 'ＬＩ', '多年前', '何况', '比方', '从不', '、', '或多或少', '让', '一旦', '论', '哪样', '不已', '而且', '元／吨', '呐', '总的来说', '哪天', '共总', '别人', '而论', '尽然', '不', '即或', '哩', '－', '>', '3', '设若', '乒', '但愿', '才', '几', '按理', '乘机', '并肩', '当然', '何时', '反过来说', '何苦', '不限', '必', '赶早不赶晚', '只有', '不然', '乃', '反之亦然', '除非', '腾', '随着', '当中', '自个儿', '起首', '不定', '嘛', '赶', '就地', '策略地', '到头来', '一番', '如其', '继而', '全然', '大概', '愤然', '莫如', '，也', '不仅仅是', '比如说', '不过', ';', '总而言之', '不只', '不怕', '靠', '咚', '此外', '快要', '趁热', '有的是', '故而', '刚好', '本人', '!', '理当', '存心', '根据', '』', '依', '”，', '尽', '省得', '方能', '来不及', '从古至今', '皆可', '来得及', '给', '有些', '一切', '宁可', '也', '一般', '它', '梆', '嗳', '才能', '白', '恰逢', '出来', '竟然', '纵使', '并且', '加以', '大面儿上', '顷刻', 'Δ', '喔唷', '大', '而况', '极大', '不对', '仍然', '向', '或是', '不至于', '趁机', '啊哟', '即令', '不大', '互', '或者', '反手', '不巧', '因为', '略', 'Ψ', '从严', '使得', '＊', '那时', '[', '那边', '的确', '窃', '一一', '《', '..', '可好', '俺', '齐', '—', '打从', '~', '其中', '鉴于', '｜', '喏', '不能不', '按时', '不时', '仍旧', '起来', '虽', '借以', '达旦', '切切', '哪里', '本着', '大体', '→', '￥', '大体上', '呗', 'β', '别说', '待', '较之', '//', '当口儿', '呜', '可能', '比', '〈', '如此等等', '由', '几乎', '还', '人家', '’', '倘若', '大约', '离', '?', '此间', '／', '刚才', '”', '凝神', '据我所知', '去', '竟', '保管', '就是', '从中', '上下', '与', '自', '然后', '这就是说', '立', '既', '嘎登', '举凡', '还是', '其', '\\', '像', '该当', '如前所述', '尽可能', '嗳哪些', '间或', '越是', '，', '人民', '串行', '除却', '立时', '＄', '哈', '白白', '顶多', '归', '…', '起初', '待到', '果然', '比起', '基本', '＝｛', '任何', '望', '不能', '只是', '立马', '多多少少', '要是', '怎么办', '带', '保险', '乘虚', '从头', '不管', '故意', '其他', '换言之', '照', '弹指之间', '即是说', '不外', '满', '率然', '挨次', '嘎', '该', '趁势', '鄙人', '屡', '分期分批', '假使', '呵', '挨门挨户', '莫非', '从新', '不由得', '较为', '据称', '长此下去', '沙沙', '尚且', '打', '略为', '那么样', '纯', '姑且', '敢', '论说', '叮咚', '来', '暗中', '逢', '如期', '按照', '大凡', '到处', '不胜', '旁人', '的', '决不', '一些', ']', '何处', '及至', '顷', '其后', '这么些', '︿', '若是', ',', '尽管如此', '充其极', '＝', '-', '从而', '用', '通过', '那', '沿着', '从', '处处', '隔夜', '难说', '高低', '咦', '再说', '叮当', '×', '不得不', '接着', '陈年', '并没', '尽如人意', '二话不说', '臭', '自从', '@', '*', '■', '恰恰相反', '单纯', '且', '何必', '岂止', '要', '己', '和', '倘使', '独', '罢了', '前后', '从重', '莫不', '呼啦', '或', '切莫', '再者', '便', '自身', '只限', '大事', '纯粹', '设使', '又', '啊', '任', '：', '乃至', '连日', '自各儿', '既是', '要不然', '极', '着', '那些', '可', '到目前为止', '迟早', '按说', '全力', '人人', '况且', '非但', '过于', '［', '老大', '因此', '奋勇', '那么些', '默然', '看来', '共', '基于', '首先', '互相', '如下', '冒', '哪', '惯常', '嗬', '恰巧', '它们', '忽地', '充其量', '极端', '么', '有', '｝', '尔等', '那样', '蛮', '近来', '喂', '轰然', '啐', '慢说', '往', '当', '多多益善', '末', '全都', '屡次三番', '背地里', '恰好', '；', '八成', '她', '必定', '长期以来', '=', '看上去', '嘿', '要不', '传说', '不得', '反之则', '亲眼', '嘎嘎', '连袂', '作为', '颇', '即若', '进来', '似的', '老', '陡然', '够瞧的', '不仅', '从古到今', '何止', '联袂', '哪些', '传', '粗', '常言说得好', '此', '另行', '自家', '尽心尽力', '接连不断', '见', '不拘', '──', '刚', '断然', '总的来看', '倒是', './', '一个', '成年累月', '其余', '取道', '不了', '％', '即刻', '就算', '朝着', '恍然', '经过', '不是', '嗯', '到头', '什么', '加上', '恐怕', '敢情', '单', '$', '从此以后', '阿', '理该', '倍加', '者', '光是', '由于', '+', '各种', '不再', '藉以', '每', '综上所述', '比照', '可见', '趁便', '多年来', '简直', '其一', '能', '而言', '……', '截然', '多', '长话短说', '不起', '从无到有', '至于', '果真', '非得', '经常', '反之', '固然', '昂然', '哎哟', '等等', '等到', '毫无', '照着', '交口', '什么样', '尽心竭力', '绝对', '跟', '不曾', '『', '喀', '动辄', '近几年来', '凑巧', '除此之外', '亲身', '不管怎样', '具体说来', '之', '》', '从优', '据', '否则', '大举', '呆呆地', '偏偏', '此后', '乘势', '只要', '上来', '伙同', '不论', '来讲', '从速', '这么点儿', '归根到底', '他人', '某些', '老是', '那会儿', '社会主义', '反过来', '对于', '居然', '不下', '相对而言', '更进一步', '哼唷', '三', '三天两头', '不经意', '有关', '并没有', '虽说', '除去', '除此以外', '凭借', '打开天窗说亮话', '随', '＋', '＃', '来看', '毫无例外', '是', '不问', '恰似', '从小', '谨', '呼哧', '啥', '拿', '各式', '公然', '我们', '那儿', '结果', '吧哒', '穷年累月', '兮', '日见', '俺们', '虽则', '哗啦', '不力', '扑通', '除', '反而', '连', '很多', '于是', '是的', '常言道', '日臻', '从早到晚', '将要', '漫说', '任凭', '℃', '您', '你', '借', '不止', '大都', '乎', '瑟瑟', '但', '．', '就此', '要么', '历', '这里', '另', '＜', '借此', '既然', '日益', '替', '而是', '何尝', '更为', '当下', '哗', '年复一年', '不独', '定', '一来', '千万千万', '为什么', '假如', '全年', '即将', '理应', '这么样', '暗自', '难怪', '不亦乐乎', '哦', '各自', '|', '诸位', '具体地说', '并非', '缕缕', '马上', '咋', '不光', '不得了', '看起来', '别的', '每时每刻', '不少', ':', '<', '}', '不日', '千万', '］', '临到', '咱们', '岂非', '恰如', '近', '起', '极力', '具体来说', '不常', '为何', '正如', '看', '从来', '所', '各', '不成', '除此而外', '简而言之', '会', '砰', '截至', '都', '倍感', '宁肯', '急匆匆', '嘘', '半', '等', '把', '倒不如说', '切不可', '而后', '到', '如上', '仅仅', '个人', '倘', '继之', '咳', '彼', '当头', '甭', '连声', '反倒是', '何乐而不为', '方才', '不择手段', '或许', '当即', '因', '如常', '尽早', '何妨', '成年', '假若', '非徒', '风雨无阻', '很少', '他', '即便', '唉', '多少', 'ξ', '另一方面', '着呢', '三番五次', '#', '碰巧', '纵', '极为', '^', '敢于', '传闻', '何', '如此', '呢', '↑', '忽然', '没有', '近年来', '局外', '｛', '之所以', '第二', '大多', '尽快', '哼', '一样', '譬如', '默默地', '‘', '必须', '另方面', '对', '如次', '古来', '〕', '绝顶', '其二',
    '中',
}

STOPWORDS = STOPWORDS_EN | STOPWORDS_ZH


def normalize(text):
    return unicodedata.normalize('NFD', text)


# ------------------------------------------------------------------------------
# n元文法相关
# ------------------------------------------------------------------------------

def grams_filter(grams, mode='any'):
    """ 判断是否保留该grams
    :param grams: 由最大长度为n个words组成的list
    :param mode:
        'any': 当filtered list中存在True时返回True
        'all': 当filtered list中全为True时返回True
        'ends': 当filtered list头或尾为True时返回True
    """
    filtered = [word_filter(word) for word in grams]
    if mode == 'any':
        return any(filtered)
    elif mode == 'all':
        return all(filtered)
    elif mode == 'ends':
        return filtered[0] or filtered[-1]


def word_filter(word):
    """ 去除停用词、标点和复数结尾 """
    word = normalize(word)
    if regex.match(r'^\p{P}+$', word): return True
    if word.lower() in STOPWORDS: return True
    return False


# ------------------------------------------------------------------------------
# gram hashing 相关
# ------------------------------------------------------------------------------

def hash(gram, hash_size):
    """ 把gram hash到有限的hash_size空间 """
    return murmurhash3_32(gram, positive=True) % hash_size


# ------------------------------------------------------------------------------
# tfidf持久化相关
# ------------------------------------------------------------------------------

def save_tfidf(tfidf_path, tfidf, metadata=None):
    data = {
        'data': tfidf.data,
        'indices': tfidf.indices,
        'indptr': tfidf.indptr,
        'shape': tfidf.shape,
        'metadata': metadata,
    }
    numpy.savez(tfidf_path, **data)


def load_tfidf(tfidf_path):
    loader = numpy.load(tfidf_path)
    matrix = sparse.csr_matrix((loader['data'], loader['indices'], loader['indptr']), shape=loader['shape'])
    return matrix, loader['metadata'].item(0) if 'metadata' in loader else None


# ------------------------------------------------------------------------------
# tokenize相关
# ------------------------------------------------------------------------------

def special_char(token):
    if token == '-LRB-':
        return '('
    if token == '-RRB-':
        return ')'
    if token == '-LSB-':
        return '['
    if token == '-RSB-':
        return ']'
    if token == '-LCB-':
        return '{'
    if token == '-RCB-':
        return '}'
    return token


def build_token(DB, db_table, document_id):
    tokens = DB.get_tokens(db_table, document_id)
    if not tokens:
        return None
    words = tokens[2].split("\t")
    words_ws = tokens[3].split("\t")
    span = [s.split(",") for s in tokens[4].split("\t")]
    pos = tokens[5].split("\t")
    lemma = tokens[6].split("\t")
    ner = tokens[7].split("\t")

    assert len(words) == len(words_ws) and len(words) == len(span) and len(words) == len(pos) and len(words) == len(
        lemma) and len(words) == len(ner), "ERROR: " + str(document_id) + " " + " ".join(
        [str(len(words)), str(len(words_ws)), str(len(span)), str(len(pos)), str(len(lemma)), str(len(ner))]
    )

    data = []
    for i in range(len(words)):
        data.append((
            special_char(words[i]),
            words_ws[i],
            (span[i][0], span[i][1]),
            pos[i],
            lemma[i],
            ner[i]
        ))

    return Tokens(data)
